ConfigMap
- манифест для кубера в котором мы сохраняем инфу о нагшем прложении
(хранит конфигурацию нашего прилодения)

в volumeMounts мы говорим что мы хотим взять контейнер
с названием конфиг и замонтировать его внутрь нашего приложения
по пути mountPath

В yaml - оч важны отступы, поэтому можно чекнуть валидный ли ваш
файл через онлайн валидотор

зайти внутрь пода:
kubectl exec -it my-deployment-794c45f64c-44jjd 
bash

подключение к подам внутри коастера кубернетес
kubectl port-forward my-deployment-794c45f64c-44jjd 8005:80 &

& - говорит что нужно уйти в бэкграунд

потом сделать curl 127.0.0.1:8005
и получить ответ от пода

можно отредактировать конфигмап
kubectl edit cm my-configmap

и немного подождать когда он примениться т.к. он
быввает запаздывает 

kubectl get cm my-configmap -o yaml
посмотреть конфигмап

nginx не следит за тем чтобы прочитать отредактированный конфиг
поэтому ему нужно дать знать что нужно его перечитать

secret - пароли, сертификаты и тд
там инфа хранится в закодированном виде (не шифрованом)
серкеты были выделены в отдельную сущность чтобы распределять
разные права доступа

Существует 3 типа секретов:
generic - обычно используется (пароли)
docker-registry - данные автотризации в dpcker registry
tls - TLS сертификаты для Ingress

создание секрета:
kubectl create secret generic test --from-literal=test1=asdf
test - имя секрета
test1 - ключ
asdf - значение секрета

kubectl get secret test -o yaml
посмотреть секрет

в примере мы запихали секрет в переменную окружения
точно также можно и конфигмапы запихивать
только вместо secretKeyRef будет configMapKeyRef

сервисы:
у нас внутри кубера есть поды со случайными ip адресами и тд
и чтобы собрать все их воедино и распределять на них трафик используется такая сущность как сервисы

у сервиса есть несколько параметров, одно из них это имя сервиса
при создании сериса этому сенрвису присваивается ip адресс
и у него в манифесте пояляется поле clusterIP
также появляется список портов
первый порт это порт на котором сервис слушает запросы
targetPort порт отдает апросы

на какие поды отправляет запросы сервисы?
для этого у нас есть метки, в селекторе есть лейбл и там указано что
сервис должен отправлять запросы на все поды с лейблом app: my-app

port - принимать
targetPort - отправлять

посмотреть сервисы
kubectl get service
мы можем посмотреть какой clusterIP был выдан сервису
и мы можем постучать по этому ip и сервис должен перенаправить наш трафик
как сервис понимает в какие поды ему отправить

у нас есть такая сущность как endpoint
kubectl get ep

когда создаем сервис, аввтоматически создается сущность
эндпоинт и в этом объекте прописаны адреса подов, на которые нужно перенаправить трафик

kubectl get pod -o wide
расширенный вывод информации о подах

как же отправить трафик на service если у него ip можем меняться
да оч просто, в кластере есть dns таблица и в этой таблице
создается запись автоматом с именем сервиса и его ip
поэтому можно отправлять трафик используя имя сервиса
и через dns таблицу оно зарезолвится.

запустим под для того чтобы постучаться в сервис
kubectl run -t -i --rm --image amouat/network-utils test bash
простой image на основе alpine для тестирования сети
на время командная строчка может как будто зависнуть

можем постучаться в сервис curl my-service
и нам будут отвечать разные инстансы пода

nslookup my-service
просмотрим сетевую инфу

если посмотреть в поле Name то увидим что там написано следущее
my-service.default.svc.cluster.local
имя сервиса.неймспейс.svc.инфа о кластере
по умолчанию инфа о кластере - cluster.local

если нужно постучать в сервис, ктоторый находится  одном неймспейсе
с вами то можно просто указать имя неймспейса,
если постучать нужно в сервис с другим неймспейсом, то
указыаем имя сервиса и через точку неймспейс

но как же постучаться в кубер из интернета?
clusterIP используется только для передачи инфы внутри
кластера, мы не можем постучаться по этому ip из интернета
для получения трафика из интернета используется такая сущность как
Ingress

Ingress  - сущность, которая описывает правила доступа
к нашему приложению изввне по протоколу http

в кубере ingress не реализовван, поэтому он ставвится
как отдельная приложенька

существуют несколько ингрес контроллеров,
например nginx или cintrix

kubectl get ingress
curl <ingresshost>

трафик на поды идет случайным образом

ingress получет извне https а
далее перенаправляет по http чтобы приложение не парилось с получением сертификата и тд

как хранить постоянные данные в кубере?
чтобы данные не удаляллись, когда под умирает

1) PV - персистенд вольют, который описывает том на котором хранятся данные
2) PVC - персистед вольюм клейм - запрос на подключение постоянного тома

StarageClass - это манифест, в котором описываются параметры доступа к
провайдеру хранения данных
StoregeClass:
    NFS

как это работает:
во-первых мы создаем манифест персистед вольюм клейм
в этом манифесте мы указывваем что нам нужно 50 GB
в сторедж класее NFS

кубер смотрим, а есть ли вв его пуле прописанных вольюмов
вольюм, который удовлетворяет критериям, если да, то
подключается к поду

Том становится в статус bound (занят)

может быть такая ситуация, что вы запросили 50 GB
но в кубере приписан персистед вольют на террабайт,
не хотелось бы занимать террабайт ради 50 GB
можно ручками это все подразбить или работать с
другой абстракцией - pv provisioner

provisioner - это программака, которая умеет общаться с
проввайдером, который предоставляет тома с данными и
и создавать том с нужным размером.

параметр accessModes большей части описательный
он говорит сколько подов может читать и писать одноввременно
ReadWriteOnce - только один
ReadWriteMany - несколько
но по итогу все это контролирует провайдер.
nfs - то можно подключить много
если rbd Диск, то только один поэтому нужно чекать а что вам предоставили

kubectl get sc - посмотреть какие у нас есть storageClass-ы
если у нас в pvc он не задан, то будет использваться дефолтный

kubectl get pvc

kubectl get pv - посмотреть провизионеров

kubectl get pv <имя> -o yaml
